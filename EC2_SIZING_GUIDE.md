# ğŸ–¥ï¸ EC2 Instance Sizing Guide - Budget-Friendly Options

## Your Data: 13.19 GB CSV, 62 Million Rows

When loaded into Pandas: **~20-30 GB RAM needed**

---

## âŒ Bad News: Free Tier Won't Work

**AWS Free Tier instances:**
- `t2.micro`: 1 GB RAM âŒ (Too small)
- `t3.micro`: 1 GB RAM âŒ (Too small)

Your dataset is too large for free tier instances. **BUT** we have budget-friendly alternatives!

---

## âœ… Budget-Friendly Solutions

### **Option 1: Chunked Processing on Small Instance** ğŸ’° Best Value!

**Instance:** `m5.large` (8 GB RAM)
- **Cost**: $0.096/hour â†’ **~$2** for entire project (20 hours)
- **Works?** âœ… YES with chunked processing (already in scripts!)

```bash
# The scripts already support chunking!
python scripts/03_data_cleaning.py  # Auto-chunks if needed
python scripts/04_feature_engineering.py  # Works with chunks
```

**How it works:**
- Reads CSV in 100K row chunks (configurable)
- Processes each chunk
- Saves incrementally
- Total time: 4-6 hours (slower but works!)

---

### **Option 2: Medium Instance with Chunking** ğŸ’°ğŸ’° Good Balance

**Instance:** `m5.xlarge` (16 GB RAM)
- **Cost**: $0.192/hour â†’ **~$4** for project (20 hours)
- **Works?** âœ… YES, faster than m5.large
- **Processing time**: 2-3 hours

---

### **Option 3: Large Instance (Original Recommendation)** ğŸ’°ğŸ’°ğŸ’°

**Instance:** `m5.4xlarge` (64 GB RAM)
- **Cost**: $0.768/hour â†’ **~$15** for project (20 hours)
- **Works?** âœ… YES, loads entire dataset in memory
- **Processing time**: 1-2 hours
- **Only needed if**: You want fastest processing

---

### **Option 4: Spot Instances (70% Cheaper!)** ğŸ’° RECOMMENDED!

Use **Spot Instances** for any of the above:

| Instance | On-Demand | Spot Price | Savings |
|----------|-----------|------------|---------|
| m5.large | $0.096/hr | ~$0.029/hr | 70% off |
| m5.xlarge | $0.192/hr | ~$0.058/hr | 70% off |
| m5.2xlarge | $0.384/hr | ~$0.115/hr | 70% off |
| m5.4xlarge | $0.768/hr | ~$0.230/hr | 70% off |

**20-hour project costs:**
- m5.large Spot: **$0.58** ğŸ‰
- m5.xlarge Spot: **$1.16** ğŸ‰
- m5.4xlarge Spot: **$4.60** ğŸ‰

**Caveat**: Spot instances can be interrupted, but for batch processing this is fine!

---

### **Option 5: Process Locally (FREE!)** ğŸ’° $0

If your computer has 32+ GB RAM:

```bash
# Just run locally - no EC2 needed!
python scripts/03_data_cleaning.py --input spotify_data.csv
python scripts/04_feature_engineering.py
python ml_models/popularity_model.py
```

**Pros:**
- âœ… Completely free
- âœ… No AWS setup needed
- âœ… Keep data local

**Cons:**
- âŒ Might take 6-12 hours
- âŒ Ties up your computer
- âŒ Need good RAM (32GB+)

---

### **Option 6: Subsample the Data** ğŸ’° $0-2

Work with 10% of data (6 million rows â‰ˆ 1.3 GB):

```bash
# Create 10% sample
head -n 6200001 spotify_data.csv > spotify_sample.csv

# Use any small instance (even free tier!)
# t2.medium (4 GB RAM) - $0.0464/hr = $0.93 for project
```

**Pros:**
- âœ… Very cheap
- âœ… Faster iteration
- âœ… Great for learning/testing

**Cons:**
- âŒ Not full dataset insights
- âŒ ML models less accurate

---

## ğŸ¯ My Recommendation

### For Budget-Conscious (Best Value):

**Use m5.large with Spot pricing**
```
Cost: $0.029/hr Ã— 20 hours = $0.58
Total with S3: ~$2
```

**Why?**
- âœ… Super cheap (<$1!)
- âœ… Scripts handle chunking automatically
- âœ… You'll learn big data techniques (chunking, memory management)
- âœ… Still processes full 62M rows

---

### For Learning Experience:

**Use m5.xlarge with Spot pricing**
```
Cost: $0.058/hr Ã— 15 hours = $0.87
Total with S3: ~$2.50
```

**Why?**
- âœ… Still very cheap
- âœ… Faster than m5.large
- âœ… More comfortable RAM headroom
- âœ… Better for ML training

---

## ğŸ“‹ How to Enable Chunked Processing

Good news: **Already built into the scripts!** Just use the flags:

```bash
# Auto-detect and use chunking if needed
python scripts/03_data_cleaning.py

# Or force chunking with explicit size
python scripts/03_data_cleaning.py --chunk-size 100000
```

The scripts will:
1. Try to load full data
2. If memory error â†’ automatically switch to chunking
3. Process 100K rows at a time
4. Save results incrementally

---

## ğŸš€ Launching Spot Instances

### Via AWS Console:
1. Go to EC2 â†’ Launch Instance
2. Choose instance type (m5.large or m5.xlarge)
3. **Request Type**: Select "Spot Instances" âœ…
4. Set max price: $0.10/hr (way above spot price for safety)
5. Launch!

### Via AWS CLI:
```bash
aws ec2 request-spot-instances \
  --spot-price "0.10" \
  --instance-count 1 \
  --type "one-time" \
  --launch-specification \
    "ImageId=ami-0c55b159cbfafe1f0,\
     InstanceType=m5.large,\
     KeyName=your-key,\
     SecurityGroupIds=sg-xxxxx"
```

---

## ğŸ’¡ Cost Comparison Summary

| Approach | Instance | Type | Hours | Total Cost |
|----------|----------|------|-------|------------|
| **Best Value** | m5.large | Spot | 20 | **~$2** âœ… |
| Good Balance | m5.xlarge | Spot | 15 | **~$2.50** âœ… |
| Fast | m5.2xlarge | Spot | 10 | **~$3.50** |
| Fastest | m5.4xlarge | Spot | 10 | **~$5** |
| Learning Only | t2.medium + 10% sample | On-Demand | 5 | **~$1** |
| DIY | Local machine | - | - | **$0** âœ… |

*(All costs include S3 storage/transfer)*

---

## âš ï¸ Memory Management Tips

### If You Get Out of Memory Errors:

```python
# Option 1: Use smaller chunks
python scripts/03_data_cleaning.py --chunk-size 50000

# Option 2: Process subset first
python scripts/03_data_cleaning.py --sample 1000000  # 1M rows

# Option 3: Use data types optimization
# (already in scripts - converts to optimal dtypes)
```

### Monitor Memory Usage:

```bash
# On EC2, check memory
free -h

# While script runs
watch -n 1 free -h
```

---

## ğŸ“ What You'll Learn

### Using Small Instance (m5.large):
- âœ… Chunked data processing
- âœ… Memory-efficient programming
- âœ… Real big data constraints
- âœ… Optimization techniques

### Using Large Instance (m5.4xlarge):
- âœ… In-memory processing
- âœ… Faster iteration
- âœ… Full dataset in RAM

Both are valuable learning experiences!

---

## ğŸ“Š Processing Time Estimates

| Instance | RAM | Load Time | Clean | Features | ML | Total |
|----------|-----|-----------|-------|----------|-----|-------|
| m5.large | 8 GB | 2 hrs | 1.5 hrs | 1 hr | 2 hrs | ~6.5 hrs |
| m5.xlarge | 16 GB | 1 hr | 1 hr | 45 min | 1.5 hrs | ~4 hrs |
| m5.2xlarge | 32 GB | 30 min | 30 min | 30 min | 1 hr | ~2.5 hrs |
| m5.4xlarge | 64 GB | 15 min | 20 min | 20 min | 45 min | ~1.5 hrs |

---

## âœ… Final Recommendation

**For someone watching budget:**

```
Instance: m5.xlarge (Spot)
Cost: ~$2.50 total
Time: ~4 hours processing
```

This gives you:
- âœ… Reasonable speed
- âœ… Very low cost
- âœ… Processes full dataset
- âœ… Good learning experience

**Launch command:**
```bash
# Request spot instance
aws ec2 request-spot-instances \
  --spot-price "0.10" \
  --instance-count 1 \
  --type "one-time" \
  --launch-specification file://spot-spec.json
```

---

**Bottom line: You can do this entire project for under $3! ğŸ‰**

